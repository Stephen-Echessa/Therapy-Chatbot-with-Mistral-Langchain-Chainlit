{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('../data/Reading 2 Text Analytics for Beginners using NLTK_240116_161801.pdf')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text Analytics for Beginners using NLTK \\nReference: https://www.datacamp.com/community/tutor ials/text-analytics-beginners-nltk \\nIn today\\'s area of internet and online services, da ta is generating at incredible speed and amount. \\nGenerally, Data analyst, engineer, and scientists a re handling relational or tabular data. These \\ntabular data columns have either numerical or categ orical data. Generated data has a variety of \\nstructures such as text, image, audio, and video. O nline activities such as articles, website text, \\nblog posts, social media posts are generating unstr uctured textual data. Corporate and business \\nneed to analyze textual data to understand customer  activities, opinion, and feedback to \\nsuccessfully derive their business. To compete with  big textual data, text analytics is evolving at a \\nfaster rate than ever before. \\nIn this tutorial, you are going to cover the follow ing topics: \\n\\uf0b7 Text Analytics and NLP \\n\\uf0b7 Compare Text Analytics, NLP and Text Mining \\no Text Analysis Operations using NLTK \\no Tokenization \\no Stopwords \\no Lexicon Normalization such as Stemming and Lemmatiz ation  \\no POS Tagging \\nText Analysis Operations using NLTK \\nNLTK is a powerful Python package that provides a s et of diverse natural languages algorithms. \\nIt is free, opensource, easy to use, large communit y, and well documented. NLTK consists of the \\nmost common algorithms such as tokenizing, part-of- speech tagging, stemming, sentiment \\nanalysis, topic segmentation, and named entity reco gnition. NLTK helps the computer to \\nanalysis, preprocess, and understand the written te xt. \\nTokenization \\nTokenization is the first step in text analytics. T he process of breaking down a text paragraph into \\nsmaller chunks such as words or sentence is called Tokenization. Token is a single entity that is \\nbuilding blocks for sentence or paragraph. \\nSentence Tokenization \\nSentence tokenizer breaks text paragraph into sente nces. \\nfrom nltk.tokenize import sent_tokenize \\ntext=\"\"\"Hello Mr. Smith, how are you doing today? T he weather is great, and \\ncity is awesome. \\nThe sky is pinkish-blue. You shouldn\\'t eat cardboar d\"\"\" \\ntokenized_text=sent_tokenize(text) \\nprint(tokenized_text) Output: \\n[\\'Hello Mr. Smith, how are you doing today?\\', \\'The weather is great, and city \\nis awesome.\\', \\'The sky is pinkish-blue.\\', \"You shou ldn\\'t eat cardboard\"] \\n \\nHere, the given text is tokenized into sentences. \\nWord Tokenization \\nWord tokenizer breaks text paragraph into words. \\nfrom nltk.tokenize import word_tokenize \\ntokenized_word=word_tokenize(text) \\nprint(tokenized_word) \\n \\nOutput: \\n[\\'Hello\\', \\'Mr.\\', \\'Smith\\', \\',\\', \\'how\\', \\'are\\', \\'you\\',  \\'doing\\', \\'today\\', \\'?\\', \\n\\'The\\', \\'weather\\', \\'is\\', \\'great\\', \\',\\', \\'and\\', \\'city\\' , \\'is\\', \\'awesome\\', \\'.\\', \\n\\'The\\', \\'sky\\', \\'is\\', \\'pinkish-blue\\', \\'.\\', \\'You\\', \\'sh ould\\', \"n\\'t\", \\'eat\\', \\n\\'cardboard\\'] \\n \\nFrequency Distribution \\nfrom nltk.probability import FreqDist \\nfdist = FreqDist(tokenized_word) \\nprint(fdist) \\n<FreqDist with 25 samples and 30 outcomes> \\nfdist.most_common(2) \\n[(\\'is\\', 3), (\\',\\', 2)] \\n# Frequency Distribution Plot \\nimport matplotlib.pyplot as plt \\nfdist.plot(30,cumulative=False) \\nplt.show() \\n \\n \\nStopwords \\nStopwords considered as noise in the text. Text may  contain stop words such as is, am, are, this, \\na, an, the, etc. \\nIn NLTK for removing stopwords, you need to create a list of stopwords and filter out your list of \\ntokens from these words. \\nfrom nltk.corpus import stopwords \\nstop_words=set(stopwords.words(\"english\")) \\nprint(stop_words) \\n \\nOutput: \\n \\n{\\'their\\', \\'then\\', \\'not\\', \\'ma\\', \\'here\\', \\'other\\', \\'wo n\\', \\'up\\', \\'weren\\', \\'being\\', \\n\\'we\\', \\'those\\', \\'an\\', \\'them\\', \\'which\\', \\'him\\', \\'so\\', \\'yourselves\\', \\'what\\', \\n\\'own\\', \\'has\\', \\'should\\', \\'above\\', \\'in\\', \\'myself\\', \\'a gainst\\', \\'that\\', \\'before\\', \\n\\'t\\', \\'just\\', \\'into\\', \\'about\\', \\'most\\', \\'d\\', \\'where\\',  \\'our\\', \\'or\\', \\'such\\', \\n\\'ours\\', \\'of\\', \\'doesn\\', \\'further\\', \\'needn\\', \\'now\\', \\' some\\', \\'too\\', \\'hasn\\', \\n\\'more\\', \\'the\\', \\'yours\\', \\'her\\', \\'below\\', \\'same\\', \\'ho w\\', \\'very\\', \\'is\\', \\'did\\', \\n\\'you\\', \\'his\\', \\'when\\', \\'few\\', \\'does\\', \\'down\\', \\'yours elf\\', \\'i\\', \\'do\\', \\'both\\', \\n\\'shan\\', \\'have\\', \\'itself\\', \\'shouldn\\', \\'through\\', \\'th emselves\\', \\'o\\', \\'didn\\', \\n\\'ve\\', \\'m\\', \\'off\\', \\'out\\', \\'but\\', \\'and\\', \\'doing\\', \\'an y\\', \\'nor\\', \\'over\\', \\'had\\', \\n\\'because\\', \\'himself\\', \\'theirs\\', \\'me\\', \\'by\\', \\'she\\', \\'whom\\', \\'hers\\', \\'re\\', \\n\\'hadn\\', \\'who\\', \\'he\\', \\'my\\', \\'if\\', \\'will\\', \\'are\\', \\'wh y\\', \\'from\\', \\'am\\', \\'with\\', \\n\\'been\\', \\'its\\', \\'ourselves\\', \\'ain\\', \\'couldn\\', \\'a\\', \\' aren\\', \\'under\\', \\'ll\\', \\'on\\', \\n\\'y\\', \\'can\\', \\'they\\', \\'than\\', \\'after\\', \\'wouldn\\', \\'eac h\\', \\'once\\', \\'mightn\\', \\n\\'for\\', \\'this\\', \\'these\\', \\'s\\', \\'only\\', \\'haven\\', \\'havi ng\\', \\'all\\', \\'don\\', \\'it\\', \\n\\'there\\', \\'until\\', \\'again\\', \\'to\\', \\'while\\', \\'be\\', \\'no \\', \\'during\\', \\'herself\\', \\n\\'as\\', \\'mustn\\', \\'between\\', \\'was\\', \\'at\\', \\'your\\', \\'wer e\\', \\'isn\\', \\'wasn\\'} \\n \\nRemoving Stopwords \\nfiltered_sent=[] \\nfor w in tokenized_sent: \\n    if w not in stop_words: \\n        filtered_sent.append(w) \\nprint(\"Tokenized Sentence:\",tokenized_sent) \\nprint(\"Filterd Sentence:\",filtered_sent) \\n \\nOutput: \\nTokenized Sentence: [\\'Hello\\', \\'Mr.\\', \\'Smith\\', \\',\\', \\'how\\', \\'are\\', \\'you\\', \\n\\'doing\\', \\'today\\', \\'?\\'] \\nFilterd Sentence: [\\'Hello\\', \\'Mr.\\', \\'Smith\\', \\',\\', \\'t oday\\', \\'?\\'] \\n \\nLexicon Normalization \\nLexicon normalization considers another type of noi se in the text. For example, connection, \\nconnected, connecting word reduce to a common word \"connect\". It reduces derivationally \\nrelated forms of a word to a common root word. Stemming \\nStemming is a process of linguistic normalization, which reduces words to their word root word \\nor chops off the derivational affixes. For example,  connection, connected, connecting word \\nreduce to a common word \"connect\". \\n# Stemming \\nfrom nltk.stem import PorterStemmer \\nfrom nltk.tokenize import sent_tokenize, word_token ize \\n \\nps = PorterStemmer() \\n \\nstemmed_words=[] \\nfor w in filtered_sent: \\n    stemmed_words.append(ps.stem(w)) \\n \\nprint(\"Filtered Sentence:\",filtered_sent) \\nprint(\"Stemmed Sentence:\",stemmed_words) \\n \\nOutput:  \\n \\nFiltered Sentence: [\\'Hello\\', \\'Mr.\\', \\'Smith\\', \\',\\', \\' today\\', \\'?\\'] \\nStemmed Sentence: [\\'hello\\', \\'mr.\\', \\'smith\\', \\',\\', \\'t oday\\', \\'?\\'] \\nLemmatization \\nLemmatization reduces words to their base word, whi ch is linguistically correct lemmas. It \\ntransforms root word with the use of vocabulary and  morphological analysis. Lemmatization is \\nusually more sophisticated than stemming. Stemmer w orks on an individual word without \\nknowledge of the context. For example, The word \"be tter\" has \"good\" as its lemma. This thing \\nwill miss by stemming because it requires a diction ary look-up. \\n#Lexicon Normalization \\n#performing stemming and Lemmatization \\n \\nfrom nltk.stem.wordnet import WordNetLemmatizer \\nlem = WordNetLemmatizer() \\n \\nfrom nltk.stem.porter import PorterStemmer \\nstem = PorterStemmer() \\n \\nword = \"flying\" \\nprint(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) \\nprint(\"Stemmed Word:\",stem.stem(word)) \\n \\n \\nOutput: \\nLemmatized Word: fly \\nStemmed Word: fli \\n \\nPOS Tagging \\nThe primary target of Part-of-Speech(POS) tagging i s to identify the grammatical group of a \\ngiven word. Whether it is a NOUN, PRONOUN, ADJECTIV E, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships  within the sentence and assigns a \\ncorresponding tag to the word. \\nsent = \"Albert Einstein was born in Ulm, Germany in  1879.\" \\ntokens=nltk.word_tokenize(sent) \\nprint(tokens) \\n \\nOutput: \\n[\\'Albert\\', \\'Einstein\\', \\'was\\', \\'born\\', \\'in\\', \\'Ulm\\', \\',\\', \\'Germany\\', \\'in\\', \\n\\'1879\\', \\'.\\'] \\n \\nnltk.pos_tag(tokens) \\n \\nOutut: \\n \\n[(\\'Albert\\', \\'NNP\\'), \\n (\\'Einstein\\', \\'NNP\\'), \\n (\\'was\\', \\'VBD\\'), \\n (\\'born\\', \\'VBN\\'), \\n (\\'in\\', \\'IN\\'), \\n (\\'Ulm\\', \\'NNP\\'), \\n (\\',\\', \\',\\'), \\n (\\'Germany\\', \\'NNP\\'), \\n (\\'in\\', \\'IN\\'), \\n (\\'1879\\', \\'CD\\'), \\n (\\'.\\', \\'.\\')] \\nPOS tagged: Albert/NNP Einstein/NNP was/VBD born/VB N in/IN Ulm/NNP ,/, \\nGermany/NNP in/IN 1879/CD ./. \\n \\n \\n '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gen = ''\n",
    "for page in data:\n",
    "    text_gen += page.page_content\n",
    "text_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_gen = text_splitter.split_text(text_gen)\n",
    "len(chunks_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Text Analytics for Beginners using NLTK \\nReference: https://www.datacamp.com/community/tutor ials/text-analytics-beginners-nltk \\nIn today's area of internet and online services, da ta is generating at incredible speed and amount. \\nGenerally, Data analyst, engineer, and scientists a re handling relational or tabular data. These \\ntabular data columns have either numerical or categ orical data. Generated data has a variety of \\nstructures such as text, image, audio, and video. O nline activities such as articles, website text, \\nblog posts, social media posts are generating unstr uctured textual data. Corporate and business \\nneed to analyze textual data to understand customer  activities, opinion, and feedback to \\nsuccessfully derive their business. To compete with  big textual data, text analytics is evolving at a \\nfaster rate than ever before. \\nIn this tutorial, you are going to cover the follow ing topics: \\n\\uf0b7 Text Analytics and NLP \\n\\uf0b7 Compare Text Analytics, NLP and Text Mining\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_gen = [Document(page_content=t) for t in chunks_gen]\n",
    "document_gen[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store doc in vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chescore/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/chescore/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/chescore/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceBgeEmbeddings(model_name='BAAI/bge-small-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chescore/anaconda3/lib/python3.11/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.4.4.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vector_store = Weaviate.from_documents(document_gen, \n",
    "                                    embedding_model, \n",
    "                                    weaviate_url = 'http://localhost:8080'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={'k': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is NLTK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o Text Analysis Operations using NLTK \n",
      "o Tokenization \n",
      "o Stopwords \n",
      "o Lexicon Normalization such as Stemming and Lemmatiz ation  \n",
      "o POS Tagging \n",
      "Text Analysis Operations using NLTK \n",
      "NLTK is a powerful Python package that provides a s et of diverse natural languages algorithms. \n",
      "It is free, opensource, easy to use, large communit y, and well documented. NLTK consists of the \n",
      "most common algorithms such as tokenizing, part-of- speech tagging, stemming, sentiment \n",
      "analysis, topic segmentation, and named entity reco gnition. NLTK helps the computer to \n",
      "analysis, preprocess, and understand the written te xt. \n",
      "Tokenization \n",
      "Tokenization is the first step in text analytics. T he process of breaking down a text paragraph into \n",
      "smaller chunks such as words or sentence is called Tokenization. Token is a single entity that is \n",
      "building blocks for sentence or paragraph. \n",
      "Sentence Tokenization \n",
      "Sentence tokenizer breaks text paragraph into sente nces. \n",
      "from nltk.tokenize import sent_tokenize\n"
     ]
    }
   ],
   "source": [
    "docs = vector_store.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model='mistral:7b-instruct-q4_K_M', temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "   ### [INST]\n",
    "   Instruction: You are an expert at answering NLP questions.\n",
    "   Here is context to help: {context}\n",
    "   ##QUESTION:\n",
    "   {question}\n",
    "   [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template='\\n   ### [INST]\\n   Instruction: You are an expert at answering NLP questions.\\n   Here is context to help: {context}\\n   ##QUESTION:\\n   {question}\\n   [/INST]\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['context', 'question'],\n",
    "    template=prompt_template\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLTK stands for Natural Language Toolkit. It is a popular Python library used for natural language processing (NLP) tasks such as tokenization, stemming, tagging, parsing, and classification. NLTK provides a wide range of tools and resources for working with text data, including pre-trained models, corpora, and utilities for data cleaning and preprocessing. It is widely used in academia and industry for tasks such as sentiment analysis, machine translation, and information extraction.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' NLTK stands for Natural Language Toolkit. It is a powerful Python package that provides a set of diverse natural language algorithms. NLTK is free, opensource, easy to use, has a large community, and is well documented. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition. NLTK helps computers analyze, preprocess, and understand written text.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
