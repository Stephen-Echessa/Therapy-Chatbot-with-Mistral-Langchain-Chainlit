{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('./data/Reading 2 Text Analytics for Beginners using NLTK_240116_161801.pdf')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text Analytics for Beginners using NLTK \\nReference: https://www.datacamp.com/community/tutor ials/text-analytics-beginners-nltk \\nIn today\\'s area of internet and online services, da ta is generating at incredible speed and amount. \\nGenerally, Data analyst, engineer, and scientists a re handling relational or tabular data. These \\ntabular data columns have either numerical or categ orical data. Generated data has a variety of \\nstructures such as text, image, audio, and video. O nline activities such as articles, website text, \\nblog posts, social media posts are generating unstr uctured textual data. Corporate and business \\nneed to analyze textual data to understand customer  activities, opinion, and feedback to \\nsuccessfully derive their business. To compete with  big textual data, text analytics is evolving at a \\nfaster rate than ever before. \\nIn this tutorial, you are going to cover the follow ing topics: \\n\\uf0b7 Text Analytics and NLP \\n\\uf0b7 Compare Text Analytics, NLP and Text Mining \\no Text Analysis Operations using NLTK \\no Tokenization \\no Stopwords \\no Lexicon Normalization such as Stemming and Lemmatiz ation  \\no POS Tagging \\nText Analysis Operations using NLTK \\nNLTK is a powerful Python package that provides a s et of diverse natural languages algorithms. \\nIt is free, opensource, easy to use, large communit y, and well documented. NLTK consists of the \\nmost common algorithms such as tokenizing, part-of- speech tagging, stemming, sentiment \\nanalysis, topic segmentation, and named entity reco gnition. NLTK helps the computer to \\nanalysis, preprocess, and understand the written te xt. \\nTokenization \\nTokenization is the first step in text analytics. T he process of breaking down a text paragraph into \\nsmaller chunks such as words or sentence is called Tokenization. Token is a single entity that is \\nbuilding blocks for sentence or paragraph. \\nSentence Tokenization \\nSentence tokenizer breaks text paragraph into sente nces. \\nfrom nltk.tokenize import sent_tokenize \\ntext=\"\"\"Hello Mr. Smith, how are you doing today? T he weather is great, and \\ncity is awesome. \\nThe sky is pinkish-blue. You shouldn\\'t eat cardboar d\"\"\" \\ntokenized_text=sent_tokenize(text) \\nprint(tokenized_text) Output: \\n[\\'Hello Mr. Smith, how are you doing today?\\', \\'The weather is great, and city \\nis awesome.\\', \\'The sky is pinkish-blue.\\', \"You shou ldn\\'t eat cardboard\"] \\n \\nHere, the given text is tokenized into sentences. \\nWord Tokenization \\nWord tokenizer breaks text paragraph into words. \\nfrom nltk.tokenize import word_tokenize \\ntokenized_word=word_tokenize(text) \\nprint(tokenized_word) \\n \\nOutput: \\n[\\'Hello\\', \\'Mr.\\', \\'Smith\\', \\',\\', \\'how\\', \\'are\\', \\'you\\',  \\'doing\\', \\'today\\', \\'?\\', \\n\\'The\\', \\'weather\\', \\'is\\', \\'great\\', \\',\\', \\'and\\', \\'city\\' , \\'is\\', \\'awesome\\', \\'.\\', \\n\\'The\\', \\'sky\\', \\'is\\', \\'pinkish-blue\\', \\'.\\', \\'You\\', \\'sh ould\\', \"n\\'t\", \\'eat\\', \\n\\'cardboard\\'] \\n \\nFrequency Distribution \\nfrom nltk.probability import FreqDist \\nfdist = FreqDist(tokenized_word) \\nprint(fdist) \\n<FreqDist with 25 samples and 30 outcomes> \\nfdist.most_common(2) \\n[(\\'is\\', 3), (\\',\\', 2)] \\n# Frequency Distribution Plot \\nimport matplotlib.pyplot as plt \\nfdist.plot(30,cumulative=False) \\nplt.show() \\n \\n \\nStopwords \\nStopwords considered as noise in the text. Text may  contain stop words such as is, am, are, this, \\na, an, the, etc. \\nIn NLTK for removing stopwords, you need to create a list of stopwords and filter out your list of \\ntokens from these words. \\nfrom nltk.corpus import stopwords \\nstop_words=set(stopwords.words(\"english\")) \\nprint(stop_words) \\n \\nOutput: \\n \\n{\\'their\\', \\'then\\', \\'not\\', \\'ma\\', \\'here\\', \\'other\\', \\'wo n\\', \\'up\\', \\'weren\\', \\'being\\', \\n\\'we\\', \\'those\\', \\'an\\', \\'them\\', \\'which\\', \\'him\\', \\'so\\', \\'yourselves\\', \\'what\\', \\n\\'own\\', \\'has\\', \\'should\\', \\'above\\', \\'in\\', \\'myself\\', \\'a gainst\\', \\'that\\', \\'before\\', \\n\\'t\\', \\'just\\', \\'into\\', \\'about\\', \\'most\\', \\'d\\', \\'where\\',  \\'our\\', \\'or\\', \\'such\\', \\n\\'ours\\', \\'of\\', \\'doesn\\', \\'further\\', \\'needn\\', \\'now\\', \\' some\\', \\'too\\', \\'hasn\\', \\n\\'more\\', \\'the\\', \\'yours\\', \\'her\\', \\'below\\', \\'same\\', \\'ho w\\', \\'very\\', \\'is\\', \\'did\\', \\n\\'you\\', \\'his\\', \\'when\\', \\'few\\', \\'does\\', \\'down\\', \\'yours elf\\', \\'i\\', \\'do\\', \\'both\\', \\n\\'shan\\', \\'have\\', \\'itself\\', \\'shouldn\\', \\'through\\', \\'th emselves\\', \\'o\\', \\'didn\\', \\n\\'ve\\', \\'m\\', \\'off\\', \\'out\\', \\'but\\', \\'and\\', \\'doing\\', \\'an y\\', \\'nor\\', \\'over\\', \\'had\\', \\n\\'because\\', \\'himself\\', \\'theirs\\', \\'me\\', \\'by\\', \\'she\\', \\'whom\\', \\'hers\\', \\'re\\', \\n\\'hadn\\', \\'who\\', \\'he\\', \\'my\\', \\'if\\', \\'will\\', \\'are\\', \\'wh y\\', \\'from\\', \\'am\\', \\'with\\', \\n\\'been\\', \\'its\\', \\'ourselves\\', \\'ain\\', \\'couldn\\', \\'a\\', \\' aren\\', \\'under\\', \\'ll\\', \\'on\\', \\n\\'y\\', \\'can\\', \\'they\\', \\'than\\', \\'after\\', \\'wouldn\\', \\'eac h\\', \\'once\\', \\'mightn\\', \\n\\'for\\', \\'this\\', \\'these\\', \\'s\\', \\'only\\', \\'haven\\', \\'havi ng\\', \\'all\\', \\'don\\', \\'it\\', \\n\\'there\\', \\'until\\', \\'again\\', \\'to\\', \\'while\\', \\'be\\', \\'no \\', \\'during\\', \\'herself\\', \\n\\'as\\', \\'mustn\\', \\'between\\', \\'was\\', \\'at\\', \\'your\\', \\'wer e\\', \\'isn\\', \\'wasn\\'} \\n \\nRemoving Stopwords \\nfiltered_sent=[] \\nfor w in tokenized_sent: \\n    if w not in stop_words: \\n        filtered_sent.append(w) \\nprint(\"Tokenized Sentence:\",tokenized_sent) \\nprint(\"Filterd Sentence:\",filtered_sent) \\n \\nOutput: \\nTokenized Sentence: [\\'Hello\\', \\'Mr.\\', \\'Smith\\', \\',\\', \\'how\\', \\'are\\', \\'you\\', \\n\\'doing\\', \\'today\\', \\'?\\'] \\nFilterd Sentence: [\\'Hello\\', \\'Mr.\\', \\'Smith\\', \\',\\', \\'t oday\\', \\'?\\'] \\n \\nLexicon Normalization \\nLexicon normalization considers another type of noi se in the text. For example, connection, \\nconnected, connecting word reduce to a common word \"connect\". It reduces derivationally \\nrelated forms of a word to a common root word. Stemming \\nStemming is a process of linguistic normalization, which reduces words to their word root word \\nor chops off the derivational affixes. For example,  connection, connected, connecting word \\nreduce to a common word \"connect\". \\n# Stemming \\nfrom nltk.stem import PorterStemmer \\nfrom nltk.tokenize import sent_tokenize, word_token ize \\n \\nps = PorterStemmer() \\n \\nstemmed_words=[] \\nfor w in filtered_sent: \\n    stemmed_words.append(ps.stem(w)) \\n \\nprint(\"Filtered Sentence:\",filtered_sent) \\nprint(\"Stemmed Sentence:\",stemmed_words) \\n \\nOutput:  \\n \\nFiltered Sentence: [\\'Hello\\', \\'Mr.\\', \\'Smith\\', \\',\\', \\' today\\', \\'?\\'] \\nStemmed Sentence: [\\'hello\\', \\'mr.\\', \\'smith\\', \\',\\', \\'t oday\\', \\'?\\'] \\nLemmatization \\nLemmatization reduces words to their base word, whi ch is linguistically correct lemmas. It \\ntransforms root word with the use of vocabulary and  morphological analysis. Lemmatization is \\nusually more sophisticated than stemming. Stemmer w orks on an individual word without \\nknowledge of the context. For example, The word \"be tter\" has \"good\" as its lemma. This thing \\nwill miss by stemming because it requires a diction ary look-up. \\n#Lexicon Normalization \\n#performing stemming and Lemmatization \\n \\nfrom nltk.stem.wordnet import WordNetLemmatizer \\nlem = WordNetLemmatizer() \\n \\nfrom nltk.stem.porter import PorterStemmer \\nstem = PorterStemmer() \\n \\nword = \"flying\" \\nprint(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) \\nprint(\"Stemmed Word:\",stem.stem(word)) \\n \\n \\nOutput: \\nLemmatized Word: fly \\nStemmed Word: fli \\n \\nPOS Tagging \\nThe primary target of Part-of-Speech(POS) tagging i s to identify the grammatical group of a \\ngiven word. Whether it is a NOUN, PRONOUN, ADJECTIV E, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships  within the sentence and assigns a \\ncorresponding tag to the word. \\nsent = \"Albert Einstein was born in Ulm, Germany in  1879.\" \\ntokens=nltk.word_tokenize(sent) \\nprint(tokens) \\n \\nOutput: \\n[\\'Albert\\', \\'Einstein\\', \\'was\\', \\'born\\', \\'in\\', \\'Ulm\\', \\',\\', \\'Germany\\', \\'in\\', \\n\\'1879\\', \\'.\\'] \\n \\nnltk.pos_tag(tokens) \\n \\nOutut: \\n \\n[(\\'Albert\\', \\'NNP\\'), \\n (\\'Einstein\\', \\'NNP\\'), \\n (\\'was\\', \\'VBD\\'), \\n (\\'born\\', \\'VBN\\'), \\n (\\'in\\', \\'IN\\'), \\n (\\'Ulm\\', \\'NNP\\'), \\n (\\',\\', \\',\\'), \\n (\\'Germany\\', \\'NNP\\'), \\n (\\'in\\', \\'IN\\'), \\n (\\'1879\\', \\'CD\\'), \\n (\\'.\\', \\'.\\')] \\nPOS tagged: Albert/NNP Einstein/NNP was/VBD born/VB N in/IN Ulm/NNP ,/, \\nGermany/NNP in/IN 1879/CD ./. \\n \\n \\n '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gen = ''\n",
    "for page in data:\n",
    "    text_gen += page.page_content\n",
    "text_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_gen = text_splitter.split_text(text_gen)\n",
    "len(chunks_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Text Analytics for Beginners using NLTK \\nReference: https://www.datacamp.com/community/tutor ials/text-analytics-beginners-nltk \\nIn today's area of internet and online services, da ta is generating at incredible speed and amount. \\nGenerally, Data analyst, engineer, and scientists a re handling relational or tabular data. These \\ntabular data columns have either numerical or categ orical data. Generated data has a variety of \\nstructures such as text, image, audio, and video. O nline activities such as articles, website text, \\nblog posts, social media posts are generating unstr uctured textual data. Corporate and business \\nneed to analyze textual data to understand customer  activities, opinion, and feedback to \\nsuccessfully derive their business. To compete with  big textual data, text analytics is evolving at a \\nfaster rate than ever before. \\nIn this tutorial, you are going to cover the follow ing topics: \\n\\uf0b7 Text Analytics and NLP \\n\\uf0b7 Compare Text Analytics, NLP and Text Mining\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_gen = [Document(page_content=t) for t in chunks_gen]\n",
    "document_gen[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store doc in vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chescore/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/chescore/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/chescore/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceBgeEmbeddings(model_name='BAAI/bge-small-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chescore/anaconda3/lib/python3.11/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.4.4.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vector_store = Weaviate.from_documents(document_gen, \n",
    "                                    embedding_model, \n",
    "                                    weaviate_url = 'http://localhost:8080'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={'k': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is NLTK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o Text Analysis Operations using NLTK \n",
      "o Tokenization \n",
      "o Stopwords \n",
      "o Lexicon Normalization such as Stemming and Lemmatiz ation  \n",
      "o POS Tagging \n",
      "Text Analysis Operations using NLTK \n",
      "NLTK is a powerful Python package that provides a s et of diverse natural languages algorithms. \n",
      "It is free, opensource, easy to use, large communit y, and well documented. NLTK consists of the \n",
      "most common algorithms such as tokenizing, part-of- speech tagging, stemming, sentiment \n",
      "analysis, topic segmentation, and named entity reco gnition. NLTK helps the computer to \n",
      "analysis, preprocess, and understand the written te xt. \n",
      "Tokenization \n",
      "Tokenization is the first step in text analytics. T he process of breaking down a text paragraph into \n",
      "smaller chunks such as words or sentence is called Tokenization. Token is a single entity that is \n",
      "building blocks for sentence or paragraph. \n",
      "Sentence Tokenization \n",
      "Sentence tokenizer breaks text paragraph into sente nces. \n",
      "from nltk.tokenize import sent_tokenize\n"
     ]
    }
   ],
   "source": [
    "docs = vector_store.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model='mistral:7b-instruct-q4_K_M', temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "   ### [INST]\n",
    "   Instruction: You are an expert at answering NLP questions.\n",
    "   Here is context to help: {context}\n",
    "   ##QUESTION:\n",
    "   {question}\n",
    "   [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template='\\n   ### [INST]\\n   Instruction: You are an expert at answering NLP questions.\\n   Here is context to help: {context}\\n   ##QUESTION:\\n   {question}\\n   [/INST]\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['context', 'question'],\n",
    "    template=prompt_template\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLTK stands for Natural Language Toolkit. It is a popular Python library used for natural language processing (NLP) tasks such as tokenization, stemming, tagging, parsing, and classification. NLTK provides a wide range of tools and resources for working with text data, including pre-trained models, corpora, and utilities for data cleaning and preprocessing. It is widely used in academia and industry for tasks such as sentiment analysis, machine translation, and information extraction.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' NLTK stands for Natural Language Toolkit. It is a powerful Python package that provides a set of diverse natural language algorithms. NLTK is free, opensource, easy to use, has a large community, and is well documented. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition. NLTK helps computers analyze, preprocess, and understand written text.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "standalone_llm = Ollama(model='mistral:7b-instruct-q4_K_M', temperature=0.0, repeat_penalty=1.1)\n",
    "response_llm = Ollama(model='mistral:7b-instruct-q4_K_M', temperature=0.2, repeat_penalty=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "_template = \"\"\"\n",
    "[INST] \n",
    "Given the following conversation and a follow up question, \n",
    "rephrase the follow up question to be a standalone question, in its original language, \n",
    "that can be used to query a Weaviate index. This query will be used to retrieve documents with additional context.\n",
    "\n",
    "Let me share a couple examples.\n",
    "\n",
    "If you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\n",
    "```\n",
    "Chat History:\n",
    "Follow Up Input: How is Lawrence doing?\n",
    "Standalone Question:\n",
    "How is Lawrence doing?\n",
    "```\n",
    "\n",
    "If this is the second question onwards, you should properly rephrase the question like this:\n",
    "```\n",
    "Chat History:\n",
    "Human: How is Lawrence doing?\n",
    "AI: \n",
    "Lawrence is injured and out for the season.\n",
    "Follow Up Input: What was his injury?\n",
    "Standalone Question:\n",
    "What was Lawrence's injury?\n",
    "```\n",
    "\n",
    "Now, with those examples, here is the actual chat history and input question.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\n",
    "[your response here]\n",
    "[/INST] \n",
    "\"\"\"\n",
    "\n",
    "STANDALONE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key='answer', input_key='question'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load memory to access chat history\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter('history')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standalone question step to process the question and chat history\n",
    "standalone_question = {\n",
    "    'standalone_question': {\n",
    "        'question': lambda x: x['question'],\n",
    "        'chat_history': lambda x: get_buffer_string(x['chat_history'])\n",
    "    }\n",
    "    | STANDALONE_QUESTION_PROMPT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, output the result of the CONDENSE_QUESTION_PROMPT\n",
    "output_prompt = {\n",
    "    'standalone_question_prompt_result': itemgetter('standalone_question')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine to a final chain\n",
    "standalone_query_generation_prompt = loaded_memory | standalone_question | output_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'question': 'What is NLTK'}\n",
    "memory.save_context(inputs, {'answer': 'NLTK is a library that performs NLP tasks'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\n[INST] \\nGiven the following conversation and a follow up question, \\nrephrase the follow up question to be a standalone question, in its original language, \\nthat can be used to query a Weaviate index. This query will be used to retrieve documents with additional context.\\n\\nLet me share a couple examples.\\n\\nIf you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\\n```\\nChat History:\\nFollow Up Input: How is Lawrence doing?\\nStandalone Question:\\nHow is Lawrence doing?\\n```\\n\\nIf this is the second question onwards, you should properly rephrase the question like this:\\n```\\nChat History:\\nHuman: How is Lawrence doing?\\nAI: \\nLawrence is injured and out for the season.\\nFollow Up Input: What was his injury?\\nStandalone Question:\\nWhat was Lawrence\\'s injury?\\n```\\n\\nNow, with those examples, here is the actual chat history and input question.\\nChat History:\\nHuman: What is NLTK\\nAI: NLTK is a library that performs NLP tasks\\nHuman: What is NLTK\\nAI: NLTK is a library that performs NLP tasks\\nFollow Up Input: What is it?\\nStandalone question:\\n[your response here]\\n[/INST] \\n')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {'question': 'What is it?'}\n",
    "standalone_query_generation_prompt.invoke(inputs)['standalone_question_prompt_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "standalone_query_generation_chain = (\n",
    "    loaded_memory\n",
    "    | {\n",
    "        \"question\": lambda x: x['question'],\n",
    "        'chat_history': lambda x: get_buffer_string(x['chat_history'])\n",
    "    }\n",
    "    | STANDALONE_QUESTION_PROMPT\n",
    "    | standalone_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is NLTK?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {'question': 'What is it?'}\n",
    "standalone_query_generation_chain.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "    [INST] \n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {standalone_question}\n",
    "    [/INST] \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_PROMPT = ChatPromptTemplate.from_template(template=template)\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template='{page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_documents(document_gen, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator='\\n\\n'):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in document_gen]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter('history')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "standalone_question = {\n",
    "    'standalone_question': {\n",
    "        'question': lambda x: x['question'],\n",
    "        'chat_history': lambda x: get_buffer_string(x['chat_history'])\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | standalone_llm\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"standalone_question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: combine_documents(x[\"docs\"]),\n",
    "    \"standalone_question\": itemgetter(\"standalone_question\"),\n",
    "}\n",
    "\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | output_prompt | response_llm,\n",
    "    \"standalone_question\": itemgetter(\"standalone_question\"),\n",
    "    \"context\": final_inputs[\"context\"]\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
